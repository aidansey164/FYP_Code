{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "83e239d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69cb636",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Describing what each vote value represents\n",
    "#### 1 : Yes (Y)\n",
    "#### 2 : Abstain (A)\n",
    "#### 3 : No (N)\n",
    "#### 8 : Non-participating\n",
    "#### 9 : Not eligible to participate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41171b38",
   "metadata": {},
   "source": [
    "# Objective 1. Predicting how a country will vote for a resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2cbfc6de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aidan\\AppData\\Local\\Temp\\ipykernel_23836\\2639575390.py:3: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  votes_df_2 = pd.read_csv(votes_df_2, encoding='latin-1')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loading dataset\n",
    "votes_df_2 = os.path.join(\"undataset2\", 'UNVotes-1.csv')\n",
    "votes_df_2 = pd.read_csv(votes_df_2, encoding='latin-1')\n",
    "\n",
    "# Ignoring all rows that contain a 2,8 or 9 vote\n",
    "votes_df_2 = votes_df_2.loc[(votes_df_2[\"vote\"] != 2 )]\n",
    "votes_df_2 = votes_df_2.loc[(votes_df_2[\"vote\"] != 8 )]\n",
    "votes_df_2 = votes_df_2.loc[(votes_df_2[\"vote\"] != 9 )]\n",
    "\n",
    "# Selecting the country to predict how it will vote for resolutions\n",
    "votes_df_2 = votes_df_2.loc[votes_df_2['Countryname'] == 'United States of America']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "37f0fc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No has more votes than yes\n",
      "Accuracy: 0.7763975155279503\n",
      "\t\t\t Naive Bayes report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.65      0.72       214\n",
      "           3       0.76      0.87      0.81       269\n",
      "\n",
      "    accuracy                           0.78       483\n",
      "   macro avg       0.78      0.76      0.77       483\n",
      "weighted avg       0.78      0.78      0.77       483\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df = votes_df_2\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['descr'], df['vote'], test_size=0.10, random_state=27)\n",
    "\n",
    "X = pd.concat([X_train, y_train], axis=1)\n",
    "yes = X[X.vote==1]\n",
    "no = X[X.vote==3]\n",
    "\n",
    "# Applying Downsampling on the training set\n",
    "\n",
    "df_yes = yes\n",
    "df_no = no\n",
    "\n",
    "if (len(df_yes) > len(df_no)):\n",
    "    print(\"Yes has more votes than no\")\n",
    "    df_sampled = resample(df_yes, \n",
    "                                     replace=False,     # sample with replacement\n",
    "                                     n_samples=len(df_no),    # to match minority class\n",
    "                                     random_state=27) # reproducible results\n",
    "    sampled = pd.concat([df_sampled, df_no])\n",
    "    \n",
    "if(len(df_no) > len(df_yes)):\n",
    "    print(\"No has more votes than yes\")\n",
    "    df_sampled = resample(df_no, \n",
    "                                     replace=False,     # sample with replacement\n",
    "                                     n_samples=len(df_yes),    # to match minority class\n",
    "                                     random_state=27) # reproducible results\n",
    "    sampled = pd.concat([df_yes, df_sampled])\n",
    "\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth = 500\n",
    "\n",
    "\n",
    "X_train = sampled.descr\n",
    "y_train = sampled.vote\n",
    "\n",
    "\n",
    "# Vectorizing the text data\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectors = vectorizer.fit_transform(X_train.values.astype('U').ravel())\n",
    "X_test_vectors = vectorizer.transform(X_test.values.astype('U').ravel())\n",
    "\n",
    "# Training the Naivye Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Evaluating the performance of the classifier\n",
    "y_pred = clf.predict(X_test_vectors)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\t\\t\\t Naive Bayes report:\\n\",classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93478f4",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "69b98e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7929606625258799\n",
      "216 174\n",
      "\t\t\t Logistic Regression report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.67      0.74       216\n",
      "           3       0.77      0.89      0.83       267\n",
      "\n",
      "    accuracy                           0.79       483\n",
      "   macro avg       0.80      0.78      0.78       483\n",
      "weighted avg       0.80      0.79      0.79       483\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "reg = LogisticRegression()\n",
    "reg.fit(X_train_vectors, y_train)\n",
    "y_pred_lr = reg.predict(X_test_vectors)\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(\"Accuracy:\", accuracy_lr)\n",
    "print(y_pred_lr.tolist().count(1), y_test.tolist().count(1))\n",
    "print(\"\\t\\t\\t Logistic Regression report:\\n\",classification_report(y_pred_lr,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750184bf",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "569adbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7536231884057971\n",
      "272 309\n",
      "\t\t\t Decision Tree report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.63      0.69       211\n",
      "           3       0.75      0.85      0.80       272\n",
      "\n",
      "    accuracy                           0.75       483\n",
      "   macro avg       0.76      0.74      0.74       483\n",
      "weighted avg       0.75      0.75      0.75       483\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train_vectors, y_train)\n",
    "y_pred_dt = dtc.predict(X_test_vectors)\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(\"Accuracy:\", accuracy_dt)\n",
    "print(y_pred_dt.tolist().count(3), y_test.tolist().count(3))\n",
    "print(\"\\t\\t\\t Decision Tree report:\\n\",classification_report(y_pred_dt,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2745afd4",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0ac31b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8074534161490683\n",
      "260 309\n",
      "\t\t\t Random Forest report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.87      0.68      0.77       223\n",
      "           3       0.77      0.92      0.84       260\n",
      "\n",
      "    accuracy                           0.81       483\n",
      "   macro avg       0.82      0.80      0.80       483\n",
      "weighted avg       0.82      0.81      0.80       483\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train_vectors, y_train)\n",
    "y_pred_rf = rfc.predict(X_test_vectors)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Accuracy:\", accuracy_rf)\n",
    "print(y_pred_rf.tolist().count(3), y_test.tolist().count(3))\n",
    "print(\"\\t\\t\\t Random Forest report:\\n\",classification_report(y_pred_rf,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ad0488",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "56f32342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8240165631469979\n",
      "264 309\n",
      "\t\t\t SVM report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.89      0.70      0.78       219\n",
      "           3       0.79      0.92      0.85       264\n",
      "\n",
      "    accuracy                           0.82       483\n",
      "   macro avg       0.84      0.81      0.82       483\n",
      "weighted avg       0.83      0.82      0.82       483\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "svc.fit(X_train_vectors, y_train)\n",
    "y_pred_svm = svc.predict(X_test_vectors)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(\"Accuracy:\", accuracy_svm)\n",
    "print(y_pred_svm.tolist().count(3), y_test.tolist().count(3))\n",
    "print(\"\\t\\t\\t SVM report:\\n\",classification_report(y_pred_svm,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987486ca",
   "metadata": {},
   "source": [
    "# K Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "669dd06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7971014492753623\n",
      "279 309\n",
      "\t\t\t KNN report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.69      0.74       204\n",
      "           3       0.79      0.88      0.83       279\n",
      "\n",
      "    accuracy                           0.80       483\n",
      "   macro avg       0.80      0.78      0.79       483\n",
      "weighted avg       0.80      0.80      0.79       483\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aidan\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train_vectors, y_train)\n",
    "y_pred_knn = knn.predict(X_test_vectors)\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(\"Accuracy:\", accuracy_knn)\n",
    "print(y_pred_knn.tolist().count(3), y_test.tolist().count(3))\n",
    "print(\"\\t\\t\\t KNN report:\\n\",classification_report(y_pred_knn,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fec213b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Accuracies:  {'NaiveBayes': 77.63975155279503, 'LogisticRegression': 79.29606625258799, 'DecisionTree': 75.36231884057972, 'RandomForest': 80.74534161490683, 'SVM': 82.40165631469979, 'KNN': 79.71014492753623}\n",
      "The most accurate model is:  SVM\n"
     ]
    }
   ],
   "source": [
    "list1 = [y_pred,y_pred_lr,y_pred_dt,y_pred_rf,y_pred_svm, y_pred_knn]\n",
    "d =['NaiveBayes','LogisticRegression','DecisionTree','RandomForest','SVM', 'KNN']\n",
    "accuracies={} \n",
    "k=0\n",
    "list2 = []\n",
    "for i in list1:\n",
    "    list2.append(accuracy_score(i,y_test)*100)\n",
    "for i in d:\n",
    "    accuracies[i] = list2[k]\n",
    "    k+=1\n",
    "\n",
    "print(\"All Accuracies: \", accuracies)\n",
    "print(\"The most accurate model is: \", max(accuracies, key=accuracies.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7e1eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
