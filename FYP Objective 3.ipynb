{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83e239d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69cb636",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 1 : Yes (Y)\n",
    "## 2 : Abstain (A)\n",
    "## 3 : No (N)\n",
    "## 8 : Non-participating\n",
    "## 9 : Not eligible to participate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41171b38",
   "metadata": {},
   "source": [
    "# Objective 3. Using different time periods to predict resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cbfc6de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aidan\\AppData\\Local\\Temp\\ipykernel_40980\\3942605212.py:7: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  votes_df_2 = pd.read_csv(votes_df_2, encoding='latin-1')\n"
     ]
    }
   ],
   "source": [
    "# votes_df = votes_df.loc[votes_df['state_name'] == 'United States of America']\n",
    "\n",
    "# Removing all abstain votes for now https://www.kaggle.com/datasets/unitednations/general-assembly/discussion/29187\n",
    "# votes_df = votes_df.loc[votes_df[\"vote\"] != 2]\n",
    "\n",
    "votes_df_2 = os.path.join(\"undataset2\", 'UNVotes-1.csv')\n",
    "votes_df_2 = pd.read_csv(votes_df_2, encoding='latin-1')\n",
    "\n",
    "\n",
    "votes_df_2 = votes_df_2.loc[votes_df_2['Countryname'] == 'United States of America']\n",
    "votes_df_2 = votes_df_2.loc[(votes_df_2[\"vote\"] != 2 )]\n",
    "votes_df_2 = votes_df_2.loc[(votes_df_2[\"vote\"] != 8 )]\n",
    "votes_df_2 = votes_df_2.loc[(votes_df_2[\"vote\"] != 9 )]\n",
    "\n",
    "# mask = (votes_df_2['year'] >= 1981) & (votes_df_2['year'] <= 1991)\n",
    "# votes_df_2 = votes_df_2.loc[mask]\n",
    "\n",
    "# print(votes_df_2['vote'].value_counts())\n",
    "\n",
    "# ammend = votes_df_2[['descr','vote']]\n",
    "\n",
    "# check_for_nan = ammend['descr'].isnull()\n",
    "# print (type(ammend['descr'].iloc[0]))\n",
    "\n",
    "# 2009 - 2013\n",
    "# 3    240\n",
    "# 1    128\n",
    "\n",
    "# 2017 - 2021\n",
    "# 3    146\n",
    "# 1     55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37f0fc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368\n",
      "85\n",
      "Accuracy: 0.8235294117647058\n",
      "8 19\n",
      "77 66\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "\t\t\t Naive Bayes report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.32      0.75      0.44         8\n",
      "           3       0.97      0.83      0.90        77\n",
      "\n",
      "    accuracy                           0.82        85\n",
      "   macro avg       0.64      0.79      0.67        85\n",
      "weighted avg       0.91      0.82      0.85        85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "df = votes_df_2\n",
    "train = df.loc[(df['year'] >= 2009) & (df['year'] <= 2013)]\n",
    "test = df.loc[(df['year'] >= 2018)]\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "\n",
    "X_train = train['descr']\n",
    "X_test = test['descr']\n",
    "y_train = train['vote']\n",
    "y_test = test['vote']\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df['descr'], df['vote'], test_size=0.15, random_state=27)\n",
    "\n",
    "# X = pd.concat([X_train, y_train], axis=1)\n",
    "# yes = X[X.vote==1]\n",
    "# no = X[X.vote==3]\n",
    "\n",
    "# df_yes = yes\n",
    "# df_no = no\n",
    "# print(len(df_yes), len(df_no))\n",
    "\n",
    "# if (len(df_yes) > len(df_no)):\n",
    "#     print(\"Yes has more votes than no\")\n",
    "#     df_sampled = resample(df_yes, \n",
    "#                                      replace=False,     # sample with replacement\n",
    "#                                      n_samples=len(df_no),    # to match minority class\n",
    "#                                      random_state=27) # reproducible results\n",
    "#     sampled = pd.concat([df_sampled, df_no])\n",
    "    \n",
    "# if(len(df_no) > len(df_yes)):\n",
    "#     print(\"No has more votes than yes\")\n",
    "#     df_sampled = resample(df_no, \n",
    "#                                      replace=False,     # sample with replacement\n",
    "#                                      n_samples=len(df_yes),    # to match minority class\n",
    "#                                      random_state=27) # reproducible results\n",
    "#     sampled = pd.concat([df_yes, df_sampled])\n",
    "\n",
    "\n",
    "# no_sampled = sampled.loc[(sampled[\"vote\"] == 3)]\n",
    "\n",
    "# pd.options.display.max_colwidth = 500\n",
    "# print(no_sampled.iloc[41])\n",
    "# print(len(sampled))\n",
    "\n",
    "# y_train = sampled.vote\n",
    "# X_train = sampled.descr\n",
    "\n",
    "# print(len(y_train), len(X_train))\n",
    "\n",
    "\n",
    "# Vectorize the text data\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectors = vectorizer.fit_transform(X_train.values.astype('U').ravel())\n",
    "X_test_vectors = vectorizer.transform(X_test.values.astype('U').ravel())\n",
    "\n",
    "# Train the  Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "y_pred = clf.predict(X_test_vectors)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(y_pred.tolist().count(1), y_test.tolist().count(1))\n",
    "print(y_pred.tolist().count(3), y_test.tolist().count(3))\n",
    "print(y_pred.tolist().count(2), y_test.tolist().count(2))\n",
    "print(y_pred.tolist().count(8), y_test.tolist().count(8))\n",
    "print(y_pred.tolist().count(9), y_test.tolist().count(9))\n",
    "\n",
    "print(\"\\t\\t\\t Naive Bayes report:\\n\",classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93478f4",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69b98e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.788235294117647\n",
      "17 19\n",
      "\t\t\t Logistic Regression report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.47      0.53      0.50        17\n",
      "           3       0.88      0.85      0.87        68\n",
      "\n",
      "    accuracy                           0.79        85\n",
      "   macro avg       0.68      0.69      0.68        85\n",
      "weighted avg       0.80      0.79      0.79        85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "reg = LogisticRegression(class_weight='balanced')\n",
    "reg.fit(X_train_vectors, y_train)\n",
    "y_pred_lr = reg.predict(X_test_vectors)\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(\"Accuracy:\", accuracy_lr)\n",
    "print(y_pred_lr.tolist().count(1), y_test.tolist().count(1))\n",
    "print(\"\\t\\t\\t Logistic Regression report:\\n\",classification_report(y_pred_lr,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750184bf",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "569adbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7647058823529411\n",
      "60 66\n",
      "\t\t\t Decision Tree report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.48      0.55        25\n",
      "           3       0.80      0.88      0.84        60\n",
      "\n",
      "    accuracy                           0.76        85\n",
      "   macro avg       0.72      0.68      0.69        85\n",
      "weighted avg       0.75      0.76      0.75        85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train_vectors, y_train)\n",
    "y_pred_dt = dtc.predict(X_test_vectors)\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(\"Accuracy:\", accuracy_dt)\n",
    "print(y_pred_dt.tolist().count(3), y_test.tolist().count(3))\n",
    "print(\"\\t\\t\\t Decision Tree report:\\n\",classification_report(y_pred_dt,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2745afd4",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ac31b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8588235294117647\n",
      "72 66\n",
      "\t\t\t Random Forest report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.77      0.62        13\n",
      "           3       0.95      0.88      0.91        72\n",
      "\n",
      "    accuracy                           0.86        85\n",
      "   macro avg       0.74      0.82      0.77        85\n",
      "weighted avg       0.89      0.86      0.87        85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train_vectors, y_train)\n",
    "y_pred_rf = rfc.predict(X_test_vectors)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Accuracy:\", accuracy_rf)\n",
    "print(y_pred_rf.tolist().count(3), y_test.tolist().count(3))\n",
    "print(\"\\t\\t\\t Random Forest report:\\n\",classification_report(y_pred_rf,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ad0488",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56f32342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8117647058823529\n",
      "74 66\n",
      "\t\t\t SVM report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.37      0.64      0.47        11\n",
      "           3       0.94      0.84      0.89        74\n",
      "\n",
      "    accuracy                           0.81        85\n",
      "   macro avg       0.65      0.74      0.68        85\n",
      "weighted avg       0.87      0.81      0.83        85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "svc.fit(X_train_vectors, y_train)\n",
    "y_pred_svm = svc.predict(X_test_vectors)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(\"Accuracy:\", accuracy_svm)\n",
    "print(y_pred_svm.tolist().count(3), y_test.tolist().count(3))\n",
    "print(\"\\t\\t\\t SVM report:\\n\",classification_report(y_pred_svm,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987486ca",
   "metadata": {},
   "source": [
    "# K Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "669dd06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8352941176470589\n",
      "72 66\n",
      "\t\t\t KNN report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.47      0.69      0.56        13\n",
      "           3       0.94      0.86      0.90        72\n",
      "\n",
      "    accuracy                           0.84        85\n",
      "   macro avg       0.71      0.78      0.73        85\n",
      "weighted avg       0.87      0.84      0.85        85\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aidan\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train_vectors, y_train)\n",
    "y_pred_knn = knn.predict(X_test_vectors)\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(\"Accuracy:\", accuracy_knn)\n",
    "print(y_pred_knn.tolist().count(3), y_test.tolist().count(3))\n",
    "print(\"\\t\\t\\t KNN report:\\n\",classification_report(y_pred_knn,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fec213b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Accuracies:  {'NaiveBayes': 82.35294117647058, 'LogisticRegression': 78.82352941176471, 'DecisionTree': 76.47058823529412, 'RandomForest': 85.88235294117646, 'SVM': 81.17647058823529, 'KNN': 83.52941176470588}\n",
      "The most accurate model is:  RandomForest\n"
     ]
    }
   ],
   "source": [
    "list1 = [y_pred,y_pred_lr,y_pred_dt,y_pred_rf,y_pred_svm, y_pred_knn]\n",
    "d =['NaiveBayes','LogisticRegression','DecisionTree','RandomForest','SVM', 'KNN']\n",
    "accuracies={} \n",
    "k=0\n",
    "list2 = []\n",
    "for i in list1:\n",
    "    list2.append(accuracy_score(i,y_test)*100)\n",
    "for i in d:\n",
    "    accuracies[i] = list2[k]\n",
    "    k+=1\n",
    "\n",
    "print(\"All Accuracies: \", accuracies)\n",
    "print(\"The most accurate model is: \", max(accuracies, key=accuracies.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7e1eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b55b32d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
